#!/usr/bin/env python3
"""
æµ‹è¯•ä¿®å¤ç‰ˆåˆ†æç®¡é“
éªŒè¯å‰ç½®ä¾èµ–å’Œè¾“å‡ºè·¯å¾„ä¿®å¤
"""

import unittest
import tempfile
import shutil
from pathlib import Path
import json
import pandas as pd
from review_analyzer_fixed import ReviewAnalyzerFixed

class TestFixedPipeline(unittest.TestCase):
    
    def setUp(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        self.test_dir = Path(tempfile.mkdtemp())
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        self.customer_data = pd.DataFrame({
            'review_text': [
                'Great webcam with 4K quality',
                'AI tracking works perfectly',
                'Love the whiteboard feature'
            ],
            'rating': [5, 4, 5]
        })
        
        self.competitor_data = pd.DataFrame({
            'review_text': [
                'Competitor camera is okay',
                'Not as good as others'
            ],
            'rating': [3, 2]
        })
        
        # ä¿å­˜æµ‹è¯•æ•°æ®
        self.customer_file = self.test_dir / 'customer.csv'
        self.competitor_file = self.test_dir / 'competitor.csv'
        
        self.customer_data.to_csv(self.customer_file, index=False)
        self.competitor_data.to_csv(self.competitor_file, index=False)
    
    def tearDown(self):
        """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
        shutil.rmtree(self.test_dir)
    
    def test_dependency_order(self):
        """æµ‹è¯•ä¾èµ–é¡ºåºæ˜¯å¦æ­£ç¡®"""
        analyzer = ReviewAnalyzerFixed()
        
        # æ£€æŸ¥ä¾èµ–å…³ç³»å®šä¹‰
        steps = analyzer.analysis_steps
        
        # product_typeåº”è¯¥æ²¡æœ‰ä¾èµ–
        self.assertEqual(steps['product_type']['dependencies'], [])
        
        # consumer_*æ­¥éª¤åº”è¯¥ä¾èµ–product_type
        consumer_steps = ['consumer_profile', 'consumer_scenario', 'consumer_motivation', 'consumer_love', 'unmet_needs']
        for step in consumer_steps:
            self.assertIn('product_type', steps[step]['dependencies'])
        
        # opportunityåº”è¯¥ä¾èµ–å¤šä¸ªconsumeræ­¥éª¤
        opportunity_deps = steps['opportunity']['dependencies']
        self.assertIn('consumer_love', opportunity_deps)
        self.assertIn('unmet_needs', opportunity_deps)
        self.assertIn('consumer_scenario', opportunity_deps)
    
    def test_output_directory_creation(self):
        """æµ‹è¯•è¾“å‡ºç›®å½•åˆ›å»º"""
        analyzer = ReviewAnalyzerFixed()
        
        # æ£€æŸ¥è¾“å‡ºç›®å½•æ˜¯å¦åˆ›å»º
        self.assertTrue(analyzer.output_dir.exists())
        self.assertTrue(analyzer.output_dir.is_dir())
        
        # æ£€æŸ¥ç›®å½•åæ˜¯å¦åŒ…å«æ—¶é—´æˆ³
        dir_name = analyzer.output_dir.name
        self.assertTrue(dir_name.startswith('analysis_results_'))
        self.assertTrue(len(dir_name) > len('analysis_results_'))
    
    def test_context_building(self):
        """æµ‹è¯•ä¸Šä¸‹æ–‡æ„å»º"""
        analyzer = ReviewAnalyzerFixed()
        
        # æ¨¡æ‹Ÿä¸€äº›ç»“æœ
        analyzer.results = {
            'product_type': {'json_result': {'category': 'webcams'}},
            'consumer_love': {'json_result': {'top_features': ['4K', 'AI tracking']}}
        }
        
        base_context = {
            'product_type': 'webcams',
            'customer_review_data': '[]'
        }
        
        # æµ‹è¯•opportunityæ­¥éª¤çš„ä¸Šä¸‹æ–‡æ„å»º
        context = analyzer.build_context_for_step('opportunity', base_context)
        
        # åº”è¯¥åŒ…å«æ‰€éœ€çš„å‚æ•°
        expected_params = ['product_type', 'consumer_love', 'unmet_needs', 'consumer_scenario', 'customer_review_data']
        for param in expected_params:
            self.assertIn(param, context)
    
    def test_clean_result_extraction(self):
        """æµ‹è¯•ç»“æœæ¸…ç†"""
        analyzer = ReviewAnalyzerFixed()
        
        # æµ‹è¯•æ­£å¸¸JSONç»“æœ
        normal_result = {'json_result': {'data': 'test'}}
        cleaned = analyzer.extract_clean_result(normal_result)
        self.assertEqual(cleaned, {'data': 'test'})
        
        # æµ‹è¯•é”™è¯¯ç»“æœ
        error_result = {'error': 'Test error', 'raw_output': 'some output'}
        cleaned = analyzer.extract_clean_result(error_result)
        self.assertIsNone(cleaned)
        
        # æµ‹è¯•åŸå§‹è¾“å‡º
        raw_result = {'raw_output': '{"data": "test"}'}
        cleaned = analyzer.extract_clean_result(raw_result)
        self.assertEqual(cleaned, {"data": "test"})
    
    def test_data_loading_and_cleaning(self):
        """æµ‹è¯•æ•°æ®åŠ è½½å’Œæ¸…ç†"""
        analyzer = ReviewAnalyzerFixed()
        
        # åŠ è½½æµ‹è¯•æ•°æ®
        data = analyzer.load_and_clean_data(str(self.customer_file), str(self.competitor_file))
        
        # æ£€æŸ¥æ•°æ®æ˜¯å¦æ­£ç¡®åŠ è½½
        self.assertIn('customer', data)
        self.assertIn('competitor', data)
        
        # æ£€æŸ¥æ¸…ç†åçš„æ•°æ®
        self.assertIn('customer_review', analyzer.cleaned_data)
        self.assertIn('competitor_review', analyzer.cleaned_data)
        
        # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶æ˜¯å¦åˆ›å»º
        self.assertTrue((analyzer.output_dir / 'customer_reviews_cleaned.csv').exists())
        self.assertTrue((analyzer.output_dir / 'competitor_reviews_cleaned.csv').exists())

def run_dependency_validation():
    """è¿è¡Œä¾èµ–å…³ç³»éªŒè¯"""
    print("ğŸ” éªŒè¯åˆ†ææ­¥éª¤ä¾èµ–å…³ç³»...")
    
    analyzer = ReviewAnalyzerFixed()
    steps = analyzer.analysis_steps
    
    print("\nğŸ“‹ åˆ†ææ­¥éª¤ä¾èµ–å…³ç³»:")
    for step_name, config in steps.items():
        deps = config['dependencies']
        if deps:
            print(f"  {step_name} â†’ ä¾èµ–: {', '.join(deps)}")
        else:
            print(f"  {step_name} â†’ æ— ä¾èµ–")
    
    # éªŒè¯ä¾èµ–çš„æœ‰æ•ˆæ€§
    all_steps = set(steps.keys())
    invalid_deps = []
    
    for step_name, config in steps.items():
        for dep in config['dependencies']:
            if dep not in all_steps:
                invalid_deps.append(f"{step_name} â†’ {dep}")
    
    if invalid_deps:
        print(f"\nâŒ å‘ç°æ— æ•ˆä¾èµ–:")
        for invalid in invalid_deps:
            print(f"  {invalid}")
        return False
    else:
        print(f"\nâœ… æ‰€æœ‰ä¾èµ–å…³ç³»æœ‰æ•ˆ")
        return True

def run_prompt_validation():
    """éªŒè¯promptæ–‡ä»¶ä¸­çš„å‚æ•°å ä½ç¬¦"""
    print("\nğŸ” éªŒè¯promptæ–‡ä»¶å‚æ•°...")
    
    analyzer = ReviewAnalyzerFixed()
    prompts_dir = analyzer.prompts_dir
    
    issues = []
    
    for step_name, config in analyzer.analysis_steps.items():
        prompt_file = prompts_dir / config['prompt_file']
        if not prompt_file.exists():
            issues.append(f"Promptæ–‡ä»¶ä¸å­˜åœ¨: {prompt_file}")
            continue
        
        # è¯»å–promptå†…å®¹
        with open(prompt_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æ£€æŸ¥å¿…éœ€çš„å‚æ•°
        required_params = config['context_params']
        for param in required_params:
            placeholder = f"{{{param}}}"
            if placeholder not in content:
                issues.append(f"{step_name}: ç¼ºå°‘å‚æ•°å ä½ç¬¦ {placeholder}")
    
    if issues:
        print("âŒ å‘ç°é—®é¢˜:")
        for issue in issues:
            print(f"  {issue}")
        return False
    else:
        print("âœ… æ‰€æœ‰promptæ–‡ä»¶å‚æ•°éªŒè¯é€šè¿‡")
        return True

def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("ğŸ§ª è¿è¡Œä¿®å¤ç‰ˆåˆ†æç®¡é“æµ‹è¯•...")
    print("=" * 50)
    
    # è¿è¡Œå•å…ƒæµ‹è¯•
    print("1. è¿è¡Œå•å…ƒæµ‹è¯•...")
    unittest.main(argv=[''], exit=False, verbosity=2)
    
    # è¿è¡Œä¾èµ–éªŒè¯
    print("\n2. éªŒè¯ä¾èµ–å…³ç³»...")
    dep_valid = run_dependency_validation()
    
    # è¿è¡ŒpromptéªŒè¯
    print("\n3. éªŒè¯promptæ–‡ä»¶...")
    prompt_valid = run_prompt_validation()
    
    print("\n" + "=" * 50)
    if dep_valid and prompt_valid:
        print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ä¿®å¤ç‰ˆåˆ†æç®¡é“å‡†å¤‡å°±ç»ªã€‚")
        return True
    else:
        print("âŒ æµ‹è¯•å‘ç°é—®é¢˜ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°é”™è¯¯ã€‚")
        return False

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
