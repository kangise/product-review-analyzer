#!/usr/bin/env python3
"""
Review Analyzer - AI Agent Pipeline
åˆ†æå®¢æˆ·è¯„è®ºå’Œç«äº‰å¯¹æ‰‹è¯„è®ºçš„AIå¤„ç†ç®¡é“
"""

import pandas as pd
import json
import subprocess
import os
import sys
from pathlib import Path
from typing import Dict, List, Any, Optional
import logging
from datetime import datetime

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ReviewAnalyzer:
    def __init__(self, prompts_dir: str = "agent", output_language: str = "en"):
        """
        åˆå§‹åŒ–è¯„è®ºåˆ†æå™¨
        
        Args:
            prompts_dir: å­˜æ”¾MD promptæ–‡ä»¶çš„ç›®å½•
        """
        self.prompts_dir = Path(prompts_dir)
        self.output_language = output_language
        self.results = {}  # å­˜å‚¨æ¯ä¸ªæ­¥éª¤çš„JSONç»“æœ
        self.cleaned_data = {}  # å­˜å‚¨æ¸…ç†åçš„æ•°æ®
        
        # åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„è¾“å‡ºç›®å½•
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.output_dir = Path(f"result/analysis_results_{timestamp}")
        self.output_dir.mkdir(exist_ok=True)
        
        logger.info(f"è¾“å‡ºç›®å½•åˆ›å»º: {self.output_dir}")
        
        # ç¡®ä¿promptsç›®å½•å­˜åœ¨
        if not self.prompts_dir.exists():
            raise FileNotFoundError(f"agentç›®å½•ä¸å­˜åœ¨: {self.prompts_dir}")
        
    def load_and_clean_data(self, customer_review_path: str, competitor_review_path: str) -> Dict[str, pd.DataFrame]:
        """
        åŠ è½½å¹¶æ¸…ç†CSVæ•°æ®
        
        Args:
            customer_review_path: å®¢æˆ·è¯„è®ºCSVæ–‡ä»¶è·¯å¾„
            competitor_review_path: ç«äº‰å¯¹æ‰‹è¯„è®ºCSVæ–‡ä»¶è·¯å¾„
            
        Returns:
            æ¸…ç†åçš„æ•°æ®å­—å…¸
        """
        logger.info("å¼€å§‹åŠ è½½å’Œæ¸…ç†æ•°æ®...")
        
        try:
            # åŠ è½½æ•°æ®
            customer_df = pd.read_csv(customer_review_path)
            competitor_df = pd.read_csv(competitor_review_path)
            
            logger.info(f"å®¢æˆ·è¯„è®ºåŸå§‹æ•°æ®: {len(customer_df)} æ¡")
            logger.info(f"ç«äº‰å¯¹æ‰‹è¯„è®ºåŸå§‹æ•°æ®: {len(competitor_df)} æ¡")
            
            # æ£€æŸ¥å¹¶æ ‡å‡†åŒ–åˆ—å
            if 'Review Text' in customer_df.columns:
                customer_df = customer_df.rename(columns={'Review Text': 'review_text'})
            if 'Review Text' in competitor_df.columns:
                competitor_df = competitor_df.rename(columns={'Review Text': 'review_text'})
            if 'Review Rating' in customer_df.columns:
                customer_df = customer_df.rename(columns={'Review Rating': 'rating'})
            if 'Review Rating' in competitor_df.columns:
                competitor_df = competitor_df.rename(columns={'Review Rating': 'rating'})
            
            # åªä¿ç•™å¿…è¦çš„åˆ—ï¼Œå¤§å¹…å‡å°‘æ•°æ®é‡
            required_columns = ['MP ID', 'ASIN', 'Submission Date', 'review_text', 'rating']
            
            # è¿‡æ»¤å®¢æˆ·è¯„è®ºæ•°æ®
            available_customer_cols = [col for col in required_columns if col in customer_df.columns]
            customer_df = customer_df[available_customer_cols]
            logger.info(f"å®¢æˆ·è¯„è®ºä¿ç•™åˆ—: {available_customer_cols}")
            
            # è¿‡æ»¤ç«äº‰å¯¹æ‰‹è¯„è®ºæ•°æ®  
            available_competitor_cols = [col for col in required_columns if col in competitor_df.columns]
            competitor_df = competitor_df[available_competitor_cols]
            logger.info(f"ç«äº‰å¯¹æ‰‹è¯„è®ºä¿ç•™åˆ—: {available_competitor_cols}")
            
            # åŸºäºreview_textå­—æ®µå»é‡å’Œæ¸…ç†
            if 'review_text' in customer_df.columns:
                customer_df_clean = customer_df.dropna(subset=['review_text']).drop_duplicates(subset=['review_text'], keep='first')
                logger.info(f"å®¢æˆ·è¯„è®ºæ¸…ç†å: {len(customer_df_clean)} æ¡")
            else:
                logger.warning("å®¢æˆ·è¯„è®ºæ•°æ®ä¸­æœªæ‰¾åˆ°'review_text'å­—æ®µ")
                customer_df_clean = customer_df
                
            if 'review_text' in competitor_df.columns:
                competitor_df_clean = competitor_df.dropna(subset=['review_text']).drop_duplicates(subset=['review_text'], keep='first')
                logger.info(f"ç«äº‰å¯¹æ‰‹è¯„è®ºæ¸…ç†å: {len(competitor_df_clean)} æ¡")
            else:
                logger.warning("ç«äº‰å¯¹æ‰‹è¯„è®ºæ•°æ®ä¸­æœªæ‰¾åˆ°'review_text'å­—æ®µ")
                competitor_df_clean = competitor_df
            
            # è½¬æ¢ä¸ºJSONæ ¼å¼ç”¨äºprompt
            self.cleaned_data = {
                'customer_review': customer_df_clean.to_json(orient='records', force_ascii=False),
                'competitor_review': competitor_df_clean.to_json(orient='records', force_ascii=False)
            }
            
            # ä¿å­˜æ¸…ç†åçš„æ•°æ®åˆ°è¾“å‡ºç›®å½•
            customer_df_clean.to_csv(self.output_dir / 'customer_reviews_cleaned.csv', index=False)
            competitor_df_clean.to_csv(self.output_dir / 'competitor_reviews_cleaned.csv', index=False)
            
            return {'customer': customer_df_clean, 'competitor': competitor_df_clean}
            
        except Exception as e:
            logger.error(f"æ•°æ®åŠ è½½å’Œæ¸…ç†å¤±è´¥: {str(e)}")
            raise
    
    def load_prompt(self, prompt_file: str) -> str:
        """
        ä»MDæ–‡ä»¶åŠ è½½prompt
        
        Args:
            prompt_file: promptæ–‡ä»¶å (å¦‚ 'product_type.md')
            
        Returns:
            promptå†…å®¹
        """
        prompt_path = self.prompts_dir / prompt_file
        
        if not prompt_path.exists():
            raise FileNotFoundError(f"Promptæ–‡ä»¶ä¸å­˜åœ¨: {prompt_path}")
            
        with open(prompt_path, 'r', encoding='utf-8') as f:
            return f.read()
    
    def process_prompt_template(self, prompt: str, context_data: Dict) -> str:
        """
        å¤„ç†promptæ¨¡æ¿ï¼Œæ›¿æ¢å…¶ä¸­çš„å˜é‡
        
        Args:
            prompt: åŸå§‹promptå†…å®¹
            context_data: ä¸Šä¸‹æ–‡æ•°æ®
            
        Returns:
            å¤„ç†åçš„prompt
        """
        processed_prompt = prompt
        
        # æ›¿æ¢promptä¸­çš„å˜é‡å ä½ç¬¦
        for key, value in context_data.items():
            placeholder = f"{{{key}}}"
            if placeholder in processed_prompt:
                if isinstance(value, (dict, list)):
                    # å¯¹äºå¤æ‚æ•°æ®ç±»å‹ï¼Œè½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²
                    value_str = json.dumps(value, indent=2, ensure_ascii=False)
                else:
                    value_str = str(value)
                processed_prompt = processed_prompt.replace(placeholder, value_str)
        
        return processed_prompt

    def call_q_chat(self, prompt: str, context_data: Optional[Dict] = None) -> Dict[str, Any]:
        """
        è°ƒç”¨Q Chatå¹¶è¿”å›JSONç»“æœ
        
        Args:
            prompt: è¦å‘é€ç»™Qçš„prompt
            context_data: ä¸Šä¸‹æ–‡æ•°æ®ï¼Œä¼šè¢«æ³¨å…¥åˆ°promptä¸­
            
        Returns:
            Qè¿”å›çš„JSONç»“æœ
        """
        try:
            # å¤„ç†promptæ¨¡æ¿
            if context_data:
                full_prompt = self.process_prompt_template(prompt, context_data)
            else:
                full_prompt = prompt
            
            # æ·»åŠ è¯­è¨€æŒ‡ä»¤
            if self.output_language == 'zh':
                language_instruction = "\n\n**é‡è¦ï¼šè¯·ç”¨ä¸­æ–‡è¾“å‡ºæ‰€æœ‰åˆ†æç»“æœã€‚æ‰€æœ‰å­—æ®µåä¿æŒè‹±æ–‡ï¼Œä½†å­—æ®µå€¼å’Œæè¿°å†…å®¹å¿…é¡»ç”¨ä¸­æ–‡ã€‚**"
            else:
                language_instruction = "\n\n**Important: Please output all analysis results in English.**"
            
            full_prompt += language_instruction
            
            logger.info("æ­£åœ¨è°ƒç”¨Q Chat...")
            logger.info(f"Prompté•¿åº¦: {len(full_prompt)} å­—ç¬¦")
            
            # è®°å½•ä¸Šä¸‹æ–‡æ•°æ®çš„ç»“æ„ï¼ˆç”¨äºè°ƒè¯•ï¼‰
            if context_data:
                for key, value in context_data.items():
                    if isinstance(value, list):
                        logger.info(f"ä¸Šä¸‹æ–‡å‚æ•° {key}: åˆ—è¡¨ï¼Œ{len(value)} é¡¹")
                    elif isinstance(value, dict):
                        logger.info(f"ä¸Šä¸‹æ–‡å‚æ•° {key}: å­—å…¸ï¼Œ{len(value)} é”®")
                    elif isinstance(value, str):
                        logger.info(f"ä¸Šä¸‹æ–‡å‚æ•° {key}: å­—ç¬¦ä¸²ï¼Œ{len(value)} å­—ç¬¦")
                    else:
                        logger.info(f"ä¸Šä¸‹æ–‡å‚æ•° {key}: {type(value)}")
            
            # ä½¿ç”¨ç®¡é“å¼ºåˆ¶å»é™¤ANSIé¢œè‰²ä»£ç 
            env = os.environ.copy()
            env['NO_COLOR'] = '1'
            env['TERM'] = 'dumb'
            env['FORCE_COLOR'] = '0'
            
            # é€šè¿‡ç®¡é“å’Œsedå»é™¤æ‰€æœ‰ANSIè½¬ä¹‰åºåˆ—ï¼Œå®Œå…¨ç¦ç”¨å·¥å…·
            cmd = "echo '" + full_prompt.replace("'", "'\"'\"'") + "' | q chat --no-interactive | sed 's/\x1b\[[0-9;]*[mK]//g'"
            
            result = subprocess.run(
                cmd,
                shell=True,
                capture_output=True,
                text=True,
                encoding='utf-8',
                env=env
            )
            
            if result.returncode != 0:
                logger.error(f"Q Chatè°ƒç”¨å¤±è´¥: {result.stderr}")
                return {"error": f"Q Chatè°ƒç”¨å¤±è´¥: {result.stderr}", "raw_output": ""}
            
            # å°è¯•è§£æJSONè¾“å‡º
            output = result.stdout.strip()
            logger.info(f"Q Chatè¾“å‡ºé•¿åº¦: {len(output)} å­—ç¬¦")
            
            # è®°å½•è¾“å‡ºçš„å‰å‡ è¡Œç”¨äºè°ƒè¯•
            output_lines = output.split('\n')[:5]
            logger.info(f"Q Chatè¾“å‡ºå‰5è¡Œ: {output_lines}")
            
            # æ›´æ™ºèƒ½çš„JSONæå–
            json_str = self.extract_json_from_output(output)
            
            if json_str:
                logger.info(f"æˆåŠŸæå–JSONï¼Œé•¿åº¦: {len(json_str)} å­—ç¬¦")
                try:
                    parsed_json = json.loads(json_str)
                    logger.info("JSONè§£ææˆåŠŸ")
                    return parsed_json
                except json.JSONDecodeError as e:
                    logger.error(f"JSONè§£æå¤±è´¥: {str(e)}")
                    logger.error(f"å°è¯•è§£æçš„JSONå‰500å­—ç¬¦: {json_str[:500]}")
                    return {"error": f"JSONè§£æå¤±è´¥: {str(e)}", "raw_output": output, "extracted_json": json_str}
            else:
                logger.warning("æœªæ‰¾åˆ°æœ‰æ•ˆçš„JSONè¾“å‡º")
                logger.warning(f"åŸå§‹è¾“å‡ºå‰1000å­—ç¬¦: {output[:1000]}")
                return {"error": "æœªæ‰¾åˆ°æœ‰æ•ˆçš„JSONè¾“å‡º", "raw_output": output}
                
        except Exception as e:
            logger.error(f"Q Chatè°ƒç”¨å¼‚å¸¸: {str(e)}")
            return {"error": f"Q Chatè°ƒç”¨å¼‚å¸¸: {str(e)}", "raw_output": ""}

    def extract_json_from_output(self, output: str) -> Optional[str]:
        """
        ä»è¾“å‡ºä¸­æå–JSONå†…å®¹ï¼Œæ”¯æŒå¤šç§æ ¼å¼
        
        Args:
            output: Q Chatçš„åŸå§‹è¾“å‡º
            
        Returns:
            æå–çš„JSONå­—ç¬¦ä¸²ï¼Œå¦‚æœæ²¡æ‰¾åˆ°åˆ™è¿”å›None
        """
        import re
        
        # æ›´å¼ºçš„ANSIæ¸…ç†ï¼šå¤„ç†æ‰€æœ‰å¯èƒ½çš„ANSIè½¬ä¹‰åºåˆ—
        # 1. æ¸…ç†Unicodeè½¬ä¹‰çš„ANSIåºåˆ— (\u001b[...)
        output = re.sub(r'\\u001b\[[0-9;]*[a-zA-Z]?', '', output)
        
        # 2. æ¸…ç†è¿ç»­çš„ANSIé‡ç½®åºåˆ—
        output = re.sub(r'(\\u001b\[0m)+', '', output)
        output = re.sub(r'(\\u001b\[m)+', '', output)
        
        # 3. æ¸…ç†å®é™…çš„ANSIè½¬ä¹‰åºåˆ— (\x1B[...)
        ansi_escape = re.compile(r'\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])')
        output = ansi_escape.sub('', output)
        
        # 4. æ¸…ç†å…¶ä»–æ§åˆ¶å­—ç¬¦å’ŒUnicodeæ§åˆ¶å­—ç¬¦
        output = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', output)
        
        # 5. æ¸…ç†å¯èƒ½æ®‹ç•™çš„ANSIç›¸å…³å­—ç¬¦å’Œæç¤ºç¬¦
        output = re.sub(r'\[0m|\[m|> ', '', output)
        
        # 6. æ¸…ç†å¼€å¤´çš„æç¤ºç¬¦å’Œç©ºç™½
        output = re.sub(r'^[>\s]*', '', output.strip())
        
        # 7. é¢å¤–æ¸…ç†ï¼šå¤„ç†ç‰¹æ®Šçš„ANSIæ¨¡å¼
        output = re.sub(r'\\u001b\[[0-9;]*m', '', output)  # å¤„ç†æ‰€æœ‰mç»“å°¾çš„åºåˆ—
        output = re.sub(r'\\u001b\[', '', output)  # æ¸…ç†æ®‹ç•™çš„å¼€å§‹æ ‡è®°
        
        print(f"ğŸ§¹ ANSIæ¸…ç†åçš„è¾“å‡ºé•¿åº¦: {len(output)}")
        
        # æ–¹æ³•1: å¯»æ‰¾markdownä»£ç å—ä¸­çš„JSON (æ”¹è¿›çš„æ­£åˆ™è¡¨è¾¾å¼)
        json_block_pattern = r'```(?:json)?\s*(\{.*?\})\s*```'
        json_blocks = re.findall(json_block_pattern, output, re.DOTALL | re.IGNORECASE)
        
        if json_blocks:
            print(f"ğŸ“¦ æ‰¾åˆ° {len(json_blocks)} ä¸ªJSONä»£ç å—")
            # å°è¯•è§£ææ¯ä¸ªæ‰¾åˆ°çš„JSONå—ï¼Œè¿”å›ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„
            for i, block in enumerate(reversed(json_blocks)):  # ä»æœ€åä¸€ä¸ªå¼€å§‹å°è¯•
                try:
                    json.loads(block)  # éªŒè¯JSONæœ‰æ•ˆæ€§
                    print(f"âœ… JSONä»£ç å— {len(json_blocks)-i} éªŒè¯æˆåŠŸ")
                    return block
                except json.JSONDecodeError as e:
                    print(f"âŒ JSONä»£ç å— {len(json_blocks)-i} éªŒè¯å¤±è´¥: {e}")
                    continue
        
        # æ–¹æ³•2: å¯»æ‰¾æœ€å¤§çš„å®Œæ•´JSONå¯¹è±¡ (æ”¹è¿›çš„æ‹¬å·åŒ¹é…)
        json_start = output.find('{')
        if json_start == -1:
            print("âŒ æœªæ‰¾åˆ°JSONèµ·å§‹æ ‡è®°")
            return None
            
        print(f"ğŸ” æ‰¾åˆ°JSONèµ·å§‹ä½ç½®: {json_start}")
        
        # æ‰¾åˆ°åŒ¹é…çš„ç»“æŸæ‹¬å·ï¼Œå¤„ç†å­—ç¬¦ä¸²ä¸­çš„æ‹¬å·
        brace_count = 0
        in_string = False
        escape_next = False
        json_end = json_start
        
        for i in range(json_start, len(output)):
            char = output[i]
            
            if escape_next:
                escape_next = False
                continue
                
            if char == '\\':
                escape_next = True
                continue
                
            if char == '"' and not escape_next:
                in_string = not in_string
                continue
                
            if not in_string:
                if char == '{':
                    brace_count += 1
                elif char == '}':
                    brace_count -= 1
                    if brace_count == 0:
                        json_end = i + 1
                        break
        
        if brace_count == 0:
            candidate = output[json_start:json_end]
            try:
                json.loads(candidate)  # éªŒè¯JSONæœ‰æ•ˆæ€§
                return candidate
            except json.JSONDecodeError:
                pass
        
        # æ–¹æ³•3: å¯»æ‰¾å¤šè¡ŒJSONç»“æ„ (å¤„ç†æ ¼å¼åŒ–çš„JSON)
        lines = output.split('\n')
        json_lines = []
        in_json = False
        brace_count = 0
        
        for line in lines:
            stripped = line.strip()
            if not in_json and stripped.startswith('{'):
                in_json = True
                json_lines = [line]
                brace_count = stripped.count('{') - stripped.count('}')
            elif in_json:
                json_lines.append(line)
                brace_count += stripped.count('{') - stripped.count('}')
                if brace_count <= 0:
                    break
        
        if json_lines and brace_count <= 0:
            candidate = '\n'.join(json_lines)
            try:
                json.loads(candidate)  # éªŒè¯JSONæœ‰æ•ˆæ€§
                return candidate
            except json.JSONDecodeError:
                pass
        
        return None

    def extract_clean_result(self, result: Dict[str, Any]) -> Any:
        """
        ä»Q Chatç»“æœä¸­æå–å¹²å‡€çš„åˆ†ææ•°æ®
        
        Args:
            result: Q Chatè¿”å›çš„ç»“æœå¯¹è±¡
            
        Returns:
            æå–çš„å¹²å‡€æ•°æ®ï¼Œå¦‚æœæå–å¤±è´¥åˆ™è¿”å›None
        """
        if not isinstance(result, dict):
            return result
        
        # å¦‚æœæœ‰é”™è¯¯ï¼Œè¿”å›None
        if 'error' in result:
            logger.warning(f"ç»“æœåŒ…å«é”™è¯¯: {result.get('error')}")
            return None
        
        # å¦‚æœæœ‰raw_outputä½†æ²¡æœ‰å…¶ä»–ç»“æ„åŒ–æ•°æ®ï¼Œå°è¯•ä»raw_outputä¸­æå–JSON
        if 'raw_output' in result and len(result) == 1:
            raw_output = result['raw_output']
            json_str = self.extract_json_from_output(raw_output)
            if json_str:
                try:
                    return json.loads(json_str)
                except json.JSONDecodeError:
                    logger.warning("æ— æ³•ä»raw_outputä¸­è§£æJSON")
                    return None
            return None
        
        # å¦‚æœç»“æœä¸­æœ‰raw_outputä½†è¿˜æœ‰å…¶ä»–å­—æ®µï¼Œç§»é™¤raw_outputè¿”å›å…¶ä»–å­—æ®µ
        if 'raw_output' in result:
            clean_result = {k: v for k, v in result.items() if k != 'raw_output'}
            if clean_result:
                return clean_result
        
        # ç›´æ¥è¿”å›ç»“æœ
        return result

    def prepare_context_data(self, context_data: Dict) -> Dict:
        """
        å‡†å¤‡ä¸Šä¸‹æ–‡æ•°æ®ï¼Œä¼˜åŒ–JSONåºåˆ—åŒ–
        
        Args:
            context_data: åŸå§‹ä¸Šä¸‹æ–‡æ•°æ®
            
        Returns:
            ä¼˜åŒ–åçš„ä¸Šä¸‹æ–‡æ•°æ®
        """
        optimized_data = {}
        
        for key, value in context_data.items():
            if value is None:
                # å¯¹äºNoneå€¼ï¼Œæä¾›ä¸€ä¸ªè¯´æ˜æ€§çš„å ä½ç¬¦
                optimized_data[key] = f"[{key}åˆ†æç»“æœæœªèƒ½æˆåŠŸæå–ï¼Œè¯·æ£€æŸ¥å‰åºæ­¥éª¤]"
                logger.warning(f"å‚æ•° {key} ä¸ºNoneï¼Œä½¿ç”¨å ä½ç¬¦")
            elif isinstance(value, pd.DataFrame):
                # DataFrameè½¬æ¢ä¸ºç´§å‡‘çš„å­—å…¸åˆ—è¡¨
                optimized_data[key] = value.to_dict('records')
            elif isinstance(value, list) and len(value) > 0:
                # ä¿æŒåˆ—è¡¨æ•°æ®å®Œæ•´
                optimized_data[key] = value
            elif isinstance(value, dict):
                # é€’å½’å¤„ç†åµŒå¥—å­—å…¸ï¼Œä½†è¦æ£€æŸ¥æ˜¯å¦åŒ…å«é”™è¯¯ä¿¡æ¯
                if 'error' in value:
                    optimized_data[key] = f"[{key}åˆ†æå¤±è´¥: {value.get('error')}]"
                    logger.warning(f"å‚æ•° {key} åŒ…å«é”™è¯¯ä¿¡æ¯")
                else:
                    optimized_data[key] = self.prepare_context_data(value) if any(isinstance(v, (pd.DataFrame, dict)) for v in value.values()) else value
            else:
                optimized_data[key] = value
        
        return optimized_data
    
    def run_analysis_pipeline(self, customer_review_path: str, competitor_review_path: str, product_type: str) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´çš„åˆ†æç®¡é“
        
        Args:
            customer_review_path: å®¢æˆ·è¯„è®ºCSVæ–‡ä»¶è·¯å¾„
            competitor_review_path: ç«äº‰å¯¹æ‰‹è¯„è®ºCSVæ–‡ä»¶è·¯å¾„
            product_type: äº§å“ç±»å‹ä¿¡æ¯
            
        Returns:
            æ‰€æœ‰åˆ†æç»“æœçš„å­—å…¸
        """
        logger.info("å¼€å§‹è¿è¡Œåˆ†æç®¡é“...")
        
        # 1. æ•°æ®æ¸…ç†
        self.load_and_clean_data(customer_review_path, competitor_review_path)
        
        # 2. äº§å“ç±»å‹åˆ†æ
        logger.info("æ­¥éª¤1: äº§å“ç±»å‹åˆ†æ")
        product_type_prompt = self.load_prompt('product_type.md')
        self.results['product_type'] = self.call_q_chat(
            product_type_prompt, 
            {'product_type': product_type}
        )
        # ä¿å­˜ç¬¬ä¸€æ­¥ç»“æœ
        step_file = self.output_dir / "product_type.json"
        with open(step_file, 'w', encoding='utf-8') as f:
            json.dump(self.results['product_type'], f, indent=2, ensure_ascii=False)
        logger.info(f"æ­¥éª¤1ç»“æœå·²ä¿å­˜: {step_file}")
        
        # æå–ç¬¬ä¸€æ­¥çš„å¹²å‡€ç»“æœç”¨äºåç»­æ­¥éª¤
        clean_product_type = self.extract_clean_result(self.results['product_type'])
        
        # 3. æ¶ˆè´¹è€…åˆ†æ (5ä¸ªå¹¶è¡Œæ­¥éª¤)
        consumer_prompts = [
            'consumer_profile.md',
            'consumer_scenario.md', 
            'consumer_motivation.md',
            'consumer_love.md',
            'unmet_needs.md'
        ]
        
        logger.info("æ­¥éª¤2: æ¶ˆè´¹è€…åˆ†æ")
        for prompt_file in consumer_prompts:
            prompt_name = prompt_file.replace('.md', '')
            logger.info(f"  æ‰§è¡Œ: {prompt_name}")
            
            prompt = self.load_prompt(prompt_file)
            
            # ä¸ºæ¯ä¸ªæ¶ˆè´¹è€…åˆ†ææä¾›ç›¸åº”çš„ä¸Šä¸‹æ–‡ - ä½¿ç”¨ç¬¬ä¸€æ­¥çš„JSONç»“æœ
            context = {
                'product_type': clean_product_type if clean_product_type else product_type,  # ä¼˜å…ˆä½¿ç”¨JSONç»“æœï¼Œfallbackåˆ°å­—ç¬¦ä¸²
                'customer_review_data': self.cleaned_data['customer_review']
            }
            
            # ä¼˜åŒ–ä¸Šä¸‹æ–‡æ•°æ®
            optimized_context = self.prepare_context_data(context)
            self.results[prompt_name] = self.call_q_chat(prompt, optimized_context)
            
            # ä¿å­˜æ¯ä¸ªæ¶ˆè´¹è€…åˆ†ææ­¥éª¤çš„ç»“æœ
            step_file = self.output_dir / f"{prompt_name}.json"
            with open(step_file, 'w', encoding='utf-8') as f:
                json.dump(self.results[prompt_name], f, indent=2, ensure_ascii=False)
            logger.info(f"æ­¥éª¤2.{prompt_name}ç»“æœå·²ä¿å­˜: {step_file}")
        
        # 4. æœºä¼šåˆ†æ (ä¿®å¤ä¾èµ–é—®é¢˜)
        logger.info("æ­¥éª¤3: æœºä¼šåˆ†æ")
        opportunity_prompt = self.load_prompt('opportunity.md')
        
        # æå–å¹²å‡€çš„åˆ†æç»“æœç”¨äºå‚æ•°ä¼ é€’
        clean_consumer_love = self.extract_clean_result(self.results['consumer_love'])
        clean_unmet_needs = self.extract_clean_result(self.results['unmet_needs'])
        clean_consumer_scenario = self.extract_clean_result(self.results['consumer_scenario'])
        
        opportunity_context = {
            'product_type': clean_product_type if clean_product_type else product_type,  # ä½¿ç”¨ç¬¬ä¸€æ­¥JSONç»“æœ
            'consumer_love': clean_consumer_love if clean_consumer_love else "[æ¶ˆè´¹è€…å–œçˆ±ç‚¹åˆ†æä¸å¯ç”¨]",
            'unmet_needs': clean_unmet_needs if clean_unmet_needs else "[æœªæ»¡è¶³éœ€æ±‚åˆ†æä¸å¯ç”¨]",
            'consumer_scenario': clean_consumer_scenario if clean_consumer_scenario else "[ä½¿ç”¨åœºæ™¯åˆ†æä¸å¯ç”¨]",
            'customer_review_data': self.cleaned_data['customer_review']
        }
        self.results['opportunity'] = self.call_q_chat(opportunity_prompt, self.prepare_context_data(opportunity_context))
        # ä¿å­˜æœºä¼šåˆ†æç»“æœ
        step_file = self.output_dir / "opportunity.json"
        with open(step_file, 'w', encoding='utf-8') as f:
            json.dump(self.results['opportunity'], f, indent=2, ensure_ascii=False)
        logger.info(f"æ­¥éª¤3ç»“æœå·²ä¿å­˜: {step_file}")
        
        # 5. æ˜Ÿçº§è¯„åˆ†æ ¹å› åˆ†æ (ä¿®å¤ä¾èµ–é—®é¢˜)
        logger.info("æ­¥éª¤4: æ˜Ÿçº§è¯„åˆ†æ ¹å› åˆ†æ")
        star_rating_prompt = self.load_prompt('star_rating_root_cause.md')
        star_rating_context = {
            'product_type': clean_product_type if clean_product_type else product_type,  # ä½¿ç”¨ç¬¬ä¸€æ­¥JSONç»“æœ
            'consumer_love': clean_consumer_love if clean_consumer_love else "[æ¶ˆè´¹è€…å–œçˆ±ç‚¹åˆ†æä¸å¯ç”¨]",
            'unmet_needs': clean_unmet_needs if clean_unmet_needs else "[æœªæ»¡è¶³éœ€æ±‚åˆ†æä¸å¯ç”¨]",
            'customer_review_data': self.cleaned_data['customer_review']
        }
        self.results['star_rating_root_cause'] = self.call_q_chat(star_rating_prompt, self.prepare_context_data(star_rating_context))
        # ä¿å­˜æ˜Ÿçº§è¯„åˆ†åˆ†æç»“æœ
        step_file = self.output_dir / "star_rating_root_cause.json"
        with open(step_file, 'w', encoding='utf-8') as f:
            json.dump(self.results['star_rating_root_cause'], f, indent=2, ensure_ascii=False)
        logger.info(f"æ­¥éª¤4ç»“æœå·²ä¿å­˜: {step_file}")
        
        # 6. ç«äº‰å¯¹æ‰‹åˆ†æ (æ–°çš„ä¸‰é˜¶æ®µæµç¨‹)
        logger.info("æ­¥éª¤5: ç«äº‰å¯¹æ‰‹åˆ†æ")
        
        # æå–æˆ‘æ–¹ç»´åº¦æ¸…å•
        clean_consumer_love = self.extract_clean_result(self.results['consumer_love'])
        clean_unmet_needs = self.extract_clean_result(self.results['unmet_needs'])
        clean_consumer_motivation = self.extract_clean_result(self.results['consumer_motivation'])
        
        if clean_consumer_love or clean_unmet_needs or clean_consumer_motivation:
            # æå–ç»´åº¦åˆ—è¡¨ï¼ˆåªä»æˆåŠŸçš„æ¨¡å—ä¸­æå–ï¼‰
            our_love_dimensions = []
            our_unmet_dimensions = []
            our_motivation_dimensions = []
            
            if clean_consumer_love:
                our_love_dimensions = [item["èµç¾ç‚¹"] for item in clean_consumer_love.get("æ ¸å¿ƒèµç¾ç‚¹åˆ†æ", [])]
            if clean_unmet_needs:
                our_unmet_dimensions = [item["éœ€æ±‚ç±»å‹"] for item in clean_unmet_needs.get("æœªæ»¡è¶³éœ€æ±‚åˆ†æ", [])]
            if clean_consumer_motivation:
                our_motivation_dimensions = [item["åŠ¨æœº"] for item in clean_consumer_motivation.get("å…·ä½“è´­ä¹°åŠ¨æœº", [])]
            
            logger.info(f"  æå–ç»´åº¦: å–œçˆ±ç‚¹{len(our_love_dimensions)}ä¸ª, æœªæ»¡è¶³éœ€æ±‚{len(our_unmet_dimensions)}ä¸ª, è´­ä¹°åŠ¨æœº{len(our_motivation_dimensions)}ä¸ª")
            
            # é˜¶æ®µ1: ç«å“åŸºç¡€åˆ†æ
            logger.info("  é˜¶æ®µ1: ç«å“åŸºç¡€åˆ†æ")
            competitor_base_prompt = self.load_prompt('competitor_analysis_base.md')
            competitor_base_context = {
                'our_love_dimensions': our_love_dimensions,
                'our_unmet_dimensions': our_unmet_dimensions,
                'our_motivation_dimensions': our_motivation_dimensions,
                'competitor_review_data': self.cleaned_data['competitor_review']
            }
            self.results['competitor_base'] = self.call_q_chat(competitor_base_prompt, self.prepare_context_data(competitor_base_context))
            
            # ä¿å­˜ç«å“åŸºç¡€åˆ†æç»“æœ
            step_file = self.output_dir / "competitor_base.json"
            with open(step_file, 'w', encoding='utf-8') as f:
                json.dump(self.results['competitor_base'], f, indent=2, ensure_ascii=False)
            logger.info(f"  é˜¶æ®µ1ç»“æœå·²ä¿å­˜: {step_file}")
            
            # é˜¶æ®µ2: ç«å“å¯¹æ¯”åˆ†æ
            logger.info("  é˜¶æ®µ2: ç«å“å¯¹æ¯”åˆ†æ")
            clean_competitor_base = self.extract_clean_result(self.results['competitor_base'])
            
            if clean_competitor_base:
                competitor_comparison_prompt = self.load_prompt('competitor_comparison.md')
                competitor_comparison_context = {
                    'our_consumer_love': clean_consumer_love or {"æ ¸å¿ƒèµç¾ç‚¹åˆ†æ": []},
                    'our_unmet_needs': clean_unmet_needs or {"æœªæ»¡è¶³éœ€æ±‚åˆ†æ": []},
                    'our_consumer_motivation': clean_consumer_motivation or {"å…·ä½“è´­ä¹°åŠ¨æœº": []},
                    'competitor_consumer_love': clean_competitor_base.get('ç«å“æ¶ˆè´¹è€…å–œçˆ±ç‚¹', []),
                    'competitor_unmet_needs': clean_competitor_base.get('ç«å“æœªæ»¡è¶³éœ€æ±‚', []),
                    'competitor_consumer_motivation': clean_competitor_base.get('ç«å“è´­ä¹°åŠ¨æœº', [])
                }
                self.results['competitor_comparison'] = self.call_q_chat(competitor_comparison_prompt, self.prepare_context_data(competitor_comparison_context))
                
                # ä¿å­˜ç«å“å¯¹æ¯”åˆ†æç»“æœ
                step_file = self.output_dir / "competitor_comparison.json"
                with open(step_file, 'w', encoding='utf-8') as f:
                    json.dump(self.results['competitor_comparison'], f, indent=2, ensure_ascii=False)
                logger.info(f"  é˜¶æ®µ2ç»“æœå·²ä¿å­˜: {step_file}")
            else:
                logger.warning("  ç«å“åŸºç¡€åˆ†æå¤±è´¥ï¼Œè·³è¿‡å¯¹æ¯”åˆ†æ")
                self.results['competitor_comparison'] = {"error": "ç«å“åŸºç¡€åˆ†æå¤±è´¥"}
            
            # é˜¶æ®µ3: ç«å“ç‹¬æœ‰æ´å¯Ÿ
            logger.info("  é˜¶æ®µ3: ç«å“ç‹¬æœ‰æ´å¯Ÿ")
            competitor_unique_prompt = self.load_prompt('competitor_unique_insights.md')
            all_our_dimensions = our_love_dimensions + our_unmet_dimensions + our_motivation_dimensions
            competitor_unique_context = {
                'competitor_review_data': self.cleaned_data['competitor_review'],
                'our_analyzed_dimensions': all_our_dimensions
            }
            self.results['competitor_unique'] = self.call_q_chat(competitor_unique_prompt, self.prepare_context_data(competitor_unique_context))
            
            # ä¿å­˜ç«å“ç‹¬æœ‰æ´å¯Ÿç»“æœ
            step_file = self.output_dir / "competitor_unique.json"
            with open(step_file, 'w', encoding='utf-8') as f:
                json.dump(self.results['competitor_unique'], f, indent=2, ensure_ascii=False)
            logger.info(f"  é˜¶æ®µ3ç»“æœå·²ä¿å­˜: {step_file}")
            
            # åˆå¹¶æœ€ç»ˆç«å“åˆ†æç»“æœ
            final_competitor_result = {
                "ç«å“åŸºç¡€åˆ†æ": clean_competitor_base if clean_competitor_base else {"error": "åˆ†æå¤±è´¥"},
                "ç«å“å¯¹æ¯”åˆ†æ": self.extract_clean_result(self.results['competitor_comparison']) or {"error": "åˆ†æå¤±è´¥"},
                "ç«å“ç‹¬æœ‰æ´å¯Ÿ": self.extract_clean_result(self.results['competitor_unique']) or {"error": "åˆ†æå¤±è´¥"}
            }
            self.results['competitor'] = final_competitor_result
            
        else:
            logger.warning("æˆ‘æ–¹åŸºç¡€åˆ†æå…¨éƒ¨å¤±è´¥ï¼Œè·³è¿‡ç«å“åˆ†æ")
            self.results['competitor'] = {"error": "æˆ‘æ–¹åŸºç¡€åˆ†æå…¨éƒ¨å¤±è´¥ï¼Œæ— æ³•è¿›è¡Œç«å“å¯¹æ¯”"}
        
        # ä¿å­˜æœ€ç»ˆç«å“åˆ†æç»“æœ
        step_file = self.output_dir / "competitor.json"
        with open(step_file, 'w', encoding='utf-8') as f:
            json.dump(self.results['competitor'], f, indent=2, ensure_ascii=False)
        logger.info(f"æ­¥éª¤5ç»“æœå·²ä¿å­˜: {step_file}")
        
        logger.info("åˆ†æç®¡é“å®Œæˆ!")
        return self.results
    
    def save_results(self) -> str:
        """
        ä¿å­˜æ‰€æœ‰åˆ†æç»“æœåˆ°æ—¶é—´æˆ³è¾“å‡ºç›®å½•
        
        Returns:
            è¾“å‡ºç›®å½•è·¯å¾„
        """
        # ä¿å­˜å®Œæ•´ç»“æœ
        results_file = self.output_dir / "analysis_results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜æ¯ä¸ªæ­¥éª¤çš„å•ç‹¬æ–‡ä»¶
        for step_name, result in self.results.items():
            step_file = self.output_dir / f"{step_name}.json"
            with open(step_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, indent=2, ensure_ascii=False)
        
        logger.info(f"æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {self.output_dir}")
        return str(self.output_dir)

def main():
    """ä¸»å‡½æ•° - å‘½ä»¤è¡Œæ¥å£"""
    if len(sys.argv) != 4:
        print("ä½¿ç”¨æ–¹æ³•: python review_analyzer.py <customer_review.csv> <competitor_review.csv> <product_type>")
        sys.exit(1)
    
    customer_review_path = sys.argv[1]
    competitor_review_path = sys.argv[2]
    product_type = sys.argv[3]
    
    try:
        analyzer = ReviewAnalyzer()
        results = analyzer.run_analysis_pipeline(customer_review_path, competitor_review_path, product_type)
        output_dir = analyzer.save_results()
        
        print(f"\nâœ… åˆ†æå®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
        print(f"ğŸ“Š å…±å®Œæˆ {len(results)} ä¸ªåˆ†ææ­¥éª¤")
        
        # æ˜¾ç¤ºæ‰§è¡Œæ‘˜è¦
        success_count = sum(1 for r in results.values() if not (isinstance(r, dict) and 'error' in r))
        print(f"âœ… æˆåŠŸ: {success_count}/{len(results)} ä¸ªæ­¥éª¤")
        
        print("\nğŸ“‹ åˆ†ææ­¥éª¤:")
        for step in results.keys():
            result = results[step]
            if isinstance(result, dict) and 'error' in result:
                print(f"  âŒ {step}")
            else:
                print(f"  âœ… {step}")
            
    except Exception as e:
        logger.error(f"åˆ†æå¤±è´¥: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    main()
