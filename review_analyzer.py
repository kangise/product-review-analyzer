#!/usr/bin/env python3
"""
Review Analyzer - AI Agent Pipeline
åˆ†æå®¢æˆ·è¯„è®ºå’Œç«äº‰å¯¹æ‰‹è¯„è®ºçš„AIå¤„ç†ç®¡é“
"""

import pandas as pd
import json
import subprocess
import os
import sys
from pathlib import Path
from typing import Dict, List, Any, Optional
import logging
from datetime import datetime

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ReviewAnalyzer:
    def __init__(self, prompts_dir: str = "agent", output_language: str = "en"):
        """
        åˆå§‹åŒ–è¯„è®ºåˆ†æå™¨
        
        Args:
            prompts_dir: å­˜æ”¾MD promptæ–‡ä»¶çš„ç›®å½•
        """
        self.prompts_dir = Path(prompts_dir)
        self.output_language = output_language
        self.results = {}  # å­˜å‚¨æ¯ä¸ªæ­¥éª¤çš„JSONç»“æœ
        self.cleaned_data = {}  # å­˜å‚¨æ¸…ç†åçš„æ•°æ®
        
        # åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„è¾“å‡ºç›®å½•
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.output_dir = Path(f"result/analysis_results_{timestamp}")
        self.output_dir.mkdir(exist_ok=True)
        
        logger.info(f"è¾“å‡ºç›®å½•åˆ›å»º: {self.output_dir}")
        
        # ç¡®ä¿promptsç›®å½•å­˜åœ¨
        if not self.prompts_dir.exists():
            raise FileNotFoundError(f"agentç›®å½•ä¸å­˜åœ¨: {self.prompts_dir}")
        
    def load_and_clean_data(self, customer_review_path: str, competitor_review_path: str) -> Dict[str, pd.DataFrame]:
        """
        åŠ è½½å¹¶æ¸…ç†CSVæ•°æ®
        
        Args:
            customer_review_path: å®¢æˆ·è¯„è®ºCSVæ–‡ä»¶è·¯å¾„
            competitor_review_path: ç«äº‰å¯¹æ‰‹è¯„è®ºCSVæ–‡ä»¶è·¯å¾„
            
        Returns:
            æ¸…ç†åçš„æ•°æ®å­—å…¸
        """
        logger.info("å¼€å§‹åŠ è½½å’Œæ¸…ç†æ•°æ®...")
        
        try:
            # åŠ è½½æ•°æ®
            customer_df = pd.read_csv(customer_review_path)
            competitor_df = pd.read_csv(competitor_review_path)
            
            logger.info(f"å®¢æˆ·è¯„è®ºåŸå§‹æ•°æ®: {len(customer_df)} æ¡")
            logger.info(f"ç«äº‰å¯¹æ‰‹è¯„è®ºåŸå§‹æ•°æ®: {len(competitor_df)} æ¡")
            
            # æ£€æŸ¥å¹¶æ ‡å‡†åŒ–åˆ—å
            if 'Review Text' in customer_df.columns:
                customer_df = customer_df.rename(columns={'Review Text': 'review_text'})
            if 'Review Text' in competitor_df.columns:
                competitor_df = competitor_df.rename(columns={'Review Text': 'review_text'})
            if 'Review Rating' in customer_df.columns:
                customer_df = customer_df.rename(columns={'Review Rating': 'rating'})
            if 'Review Rating' in competitor_df.columns:
                competitor_df = competitor_df.rename(columns={'Review Rating': 'rating'})
            
            # åªä¿ç•™å¿…è¦çš„åˆ—ï¼Œå¤§å¹…å‡å°‘æ•°æ®é‡
            required_columns = ['MP ID', 'ASIN', 'Submission Date', 'review_text', 'rating']
            
            # è¿‡æ»¤å®¢æˆ·è¯„è®ºæ•°æ®
            available_customer_cols = [col for col in required_columns if col in customer_df.columns]
            customer_df = customer_df[available_customer_cols]
            logger.info(f"å®¢æˆ·è¯„è®ºä¿ç•™åˆ—: {available_customer_cols}")
            
            # è¿‡æ»¤ç«äº‰å¯¹æ‰‹è¯„è®ºæ•°æ®  
            available_competitor_cols = [col for col in required_columns if col in competitor_df.columns]
            competitor_df = competitor_df[available_competitor_cols]
            logger.info(f"ç«äº‰å¯¹æ‰‹è¯„è®ºä¿ç•™åˆ—: {available_competitor_cols}")
            
            # åŸºäºreview_textå­—æ®µå»é‡å’Œæ¸…ç†
            if 'review_text' in customer_df.columns:
                customer_df_clean = customer_df.dropna(subset=['review_text']).drop_duplicates(subset=['review_text'], keep='first')
                logger.info(f"å®¢æˆ·è¯„è®ºæ¸…ç†å: {len(customer_df_clean)} æ¡")
            else:
                logger.warning("å®¢æˆ·è¯„è®ºæ•°æ®ä¸­æœªæ‰¾åˆ°'review_text'å­—æ®µ")
                customer_df_clean = customer_df
                
            if 'review_text' in competitor_df.columns:
                competitor_df_clean = competitor_df.dropna(subset=['review_text']).drop_duplicates(subset=['review_text'], keep='first')
                logger.info(f"ç«äº‰å¯¹æ‰‹è¯„è®ºæ¸…ç†å: {len(competitor_df_clean)} æ¡")
            else:
                logger.warning("ç«äº‰å¯¹æ‰‹è¯„è®ºæ•°æ®ä¸­æœªæ‰¾åˆ°'review_text'å­—æ®µ")
                competitor_df_clean = competitor_df
            
            # è½¬æ¢ä¸ºJSONæ ¼å¼ç”¨äºprompt
            self.cleaned_data = {
                'customer_review': customer_df_clean.to_json(orient='records', force_ascii=False),
                'competitor_review': competitor_df_clean.to_json(orient='records', force_ascii=False)
            }
            
            # ä¿å­˜æ¸…ç†åçš„æ•°æ®åˆ°è¾“å‡ºç›®å½•
            customer_df_clean.to_csv(self.output_dir / 'customer_reviews_cleaned.csv', index=False)
            competitor_df_clean.to_csv(self.output_dir / 'competitor_reviews_cleaned.csv', index=False)
            
            return {'customer': customer_df_clean, 'competitor': competitor_df_clean}
            
        except Exception as e:
            logger.error(f"æ•°æ®åŠ è½½å’Œæ¸…ç†å¤±è´¥: {str(e)}")
            raise
    
    def load_prompt(self, prompt_file: str) -> str:
        """
        ä»MDæ–‡ä»¶åŠ è½½prompt
        
        Args:
            prompt_file: promptæ–‡ä»¶å (å¦‚ 'product_type.md')
            
        Returns:
            promptå†…å®¹
        """
        prompt_path = self.prompts_dir / prompt_file
        
        if not prompt_path.exists():
            raise FileNotFoundError(f"Promptæ–‡ä»¶ä¸å­˜åœ¨: {prompt_path}")
            
        with open(prompt_path, 'r', encoding='utf-8') as f:
            return f.read()
    
    def process_prompt_template(self, prompt: str, context_data: Dict) -> str:
        """
        å¤„ç†promptæ¨¡æ¿ï¼Œæ›¿æ¢å…¶ä¸­çš„å˜é‡
        
        Args:
            prompt: åŸå§‹promptå†…å®¹
            context_data: ä¸Šä¸‹æ–‡æ•°æ®
            
        Returns:
            å¤„ç†åçš„prompt
        """
        processed_prompt = prompt
        
        # æ›¿æ¢promptä¸­çš„å˜é‡å ä½ç¬¦
        for key, value in context_data.items():
            placeholder = f"{{{key}}}"
            if placeholder in processed_prompt:
                if isinstance(value, (dict, list)):
                    # å¯¹äºå¤æ‚æ•°æ®ç±»å‹ï¼Œè½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²
                    value_str = json.dumps(value, indent=2, ensure_ascii=False)
                else:
                    value_str = str(value)
                processed_prompt = processed_prompt.replace(placeholder, value_str)
        
        return processed_prompt

    def call_q_chat(self, prompt: str, context_data: Optional[Dict] = None) -> Dict[str, Any]:
        """
        è°ƒç”¨Q Chatå¹¶è¿”å›JSONç»“æœ
        
        Args:
            prompt: è¦å‘é€ç»™Qçš„prompt
            context_data: ä¸Šä¸‹æ–‡æ•°æ®ï¼Œä¼šè¢«æ³¨å…¥åˆ°promptä¸­
            
        Returns:
            Qè¿”å›çš„JSONç»“æœ
        """
        try:
            # å¤„ç†promptæ¨¡æ¿
            if context_data:
                full_prompt = self.process_prompt_template(prompt, context_data)
            else:
                full_prompt = prompt
            
            # æ·»åŠ è¯­è¨€æŒ‡ä»¤
            if self.output_language == 'zh':
                language_instruction = """

**é‡è¦è¯­è¨€å’Œä¸€è‡´æ€§æŒ‡ä»¤ï¼š**
1. è¯·ç”¨ä¸­æ–‡è¾“å‡ºæ‰€æœ‰åˆ†æç»“æœï¼Œä½†ä¿æŒä¸è‹±æ–‡ç‰ˆæœ¬å®Œå…¨ç›¸åŒçš„åˆ†æé€»è¾‘å’Œåˆ†ç±»æ ‡å‡†
2. é¢‘ç‡è®¡ç®—å¿…é¡»åŸºäºç›¸åŒçš„å…³é”®è¯åŒ¹é…é€»è¾‘ï¼Œä¸å—è¯­è¨€å½±å“
3. åˆ†æç»´åº¦ã€ä¼˜å…ˆçº§æ’åºã€é‡è¦æ€§åˆ¤æ–­å¿…é¡»ä¸è‹±æ–‡ç‰ˆæœ¬ä¿æŒä¸€è‡´
4. åªæ”¹å˜è¯­è¨€è¡¨è¾¾ï¼Œä¸æ”¹å˜åˆ†æç»“æœçš„æœ¬è´¨å†…å®¹
5. æ‰€æœ‰ç™¾åˆ†æ¯”å’Œæ•°å€¼å¿…é¡»åŸºäºç›¸åŒçš„ç»Ÿè®¡æ–¹æ³•
6. ç¡®ä¿ç›¸åŒçš„æ•°æ®äº§ç”Ÿç›¸åŒçš„åˆ†æç»“æ„å’Œé¢‘ç‡åˆ†å¸ƒ
"""
            else:
                language_instruction = """

**Important Language and Consistency Instructions:**
1. Output all analysis results in English with consistent analytical logic
2. Frequency calculations must be based on the same keyword matching logic
3. Analysis dimensions, priority rankings, and importance judgments must be consistent
4. All percentages and numerical values must be based on the same statistical methods
5. Ensure the same data produces the same analytical structure and frequency distribution
"""
            
            full_prompt += language_instruction
            
            logger.info("æ­£åœ¨è°ƒç”¨Q Chat...")
            logger.info(f"Prompté•¿åº¦: {len(full_prompt)} å­—ç¬¦")
            
            # è®°å½•ä¸Šä¸‹æ–‡æ•°æ®çš„ç»“æ„ï¼ˆç”¨äºè°ƒè¯•ï¼‰
            if context_data:
                for key, value in context_data.items():
                    if isinstance(value, list):
                        logger.info(f"ä¸Šä¸‹æ–‡å‚æ•° {key}: åˆ—è¡¨ï¼Œ{len(value)} é¡¹")
                    elif isinstance(value, dict):
                        logger.info(f"ä¸Šä¸‹æ–‡å‚æ•° {key}: å­—å…¸ï¼Œ{len(value)} é”®")
                    elif isinstance(value, str):
                        logger.info(f"ä¸Šä¸‹æ–‡å‚æ•° {key}: å­—ç¬¦ä¸²ï¼Œ{len(value)} å­—ç¬¦")
                    else:
                        logger.info(f"ä¸Šä¸‹æ–‡å‚æ•° {key}: {type(value)}")
            
            # ä½¿ç”¨ç®¡é“å¼ºåˆ¶å»é™¤ANSIé¢œè‰²ä»£ç 
            env = os.environ.copy()
            env['NO_COLOR'] = '1'
            env['TERM'] = 'dumb'
            env['FORCE_COLOR'] = '0'
            
            # é€šè¿‡ç®¡é“å’Œsedå»é™¤æ‰€æœ‰ANSIè½¬ä¹‰åºåˆ—ï¼Œå®Œå…¨ç¦ç”¨å·¥å…·
            cmd = "echo '" + full_prompt.replace("'", "'\"'\"'") + "' | q chat --no-interactive | sed 's/\x1b\[[0-9;]*[mK]//g'"
            
            result = subprocess.run(
                cmd,
                shell=True,
                capture_output=True,
                text=True,
                encoding='utf-8',
                env=env
            )
            
            if result.returncode != 0:
                logger.error(f"Q Chatè°ƒç”¨å¤±è´¥: {result.stderr}")
                return {"error": f"Q Chatè°ƒç”¨å¤±è´¥: {result.stderr}", "raw_output": ""}
            
            # å°è¯•è§£æJSONè¾“å‡º
            output = result.stdout.strip()
            logger.info(f"Q Chatè¾“å‡ºé•¿åº¦: {len(output)} å­—ç¬¦")
            
            # è®°å½•è¾“å‡ºçš„å‰å‡ è¡Œç”¨äºè°ƒè¯•
            output_lines = output.split('\n')[:5]
            logger.info(f"Q Chatè¾“å‡ºå‰5è¡Œ: {output_lines}")
            
            # æ›´æ™ºèƒ½çš„JSONæå–
            json_str = self.extract_json_from_output(output)
            
            if json_str:
                logger.info(f"æˆåŠŸæå–JSONï¼Œé•¿åº¦: {len(json_str)} å­—ç¬¦")
                try:
                    parsed_json = json.loads(json_str)
                    logger.info("JSONè§£ææˆåŠŸ")
                    return parsed_json
                except json.JSONDecodeError as e:
                    logger.error(f"JSONè§£æå¤±è´¥: {str(e)}")
                    logger.error(f"å°è¯•è§£æçš„JSONå‰500å­—ç¬¦: {json_str[:500]}")
                    return {"error": f"JSONè§£æå¤±è´¥: {str(e)}", "raw_output": output, "extracted_json": json_str}
            else:
                logger.warning("æœªæ‰¾åˆ°æœ‰æ•ˆçš„JSONè¾“å‡º")
                logger.warning(f"åŸå§‹è¾“å‡ºå‰1000å­—ç¬¦: {output[:1000]}")
                return {"error": "æœªæ‰¾åˆ°æœ‰æ•ˆçš„JSONè¾“å‡º", "raw_output": output}
                
        except Exception as e:
            logger.error(f"Q Chatè°ƒç”¨å¼‚å¸¸: {str(e)}")
            return {"error": f"Q Chatè°ƒç”¨å¼‚å¸¸: {str(e)}", "raw_output": ""}

    def fix_multiline_json_strings(self, json_str: str) -> str:
        """
        ä¿®å¤JSONä¸­çš„å¤šè¡Œå­—ç¬¦ä¸²é—®é¢˜
        """
        import re
        
        # å°†å­—ç¬¦ä¸²å€¼ä¸­çš„æ¢è¡Œç¬¦æ›¿æ¢ä¸ºç©ºæ ¼ï¼Œä½†ä¿æŒJSONç»“æ„çš„æ¢è¡Œ
        lines = json_str.split('\n')
        fixed_lines = []
        in_string = False
        current_line = ""
        
        for line in lines:
            stripped = line.strip()
            
            # æ£€æŸ¥è¿™è¡Œæ˜¯å¦åœ¨å­—ç¬¦ä¸²å†…éƒ¨
            if in_string:
                # å¦‚æœåœ¨å­—ç¬¦ä¸²å†…éƒ¨ï¼Œå°†å†…å®¹æ·»åŠ åˆ°å½“å‰è¡Œï¼Œç”¨ç©ºæ ¼è¿æ¥
                current_line += " " + stripped
                # æ£€æŸ¥æ˜¯å¦ç»“æŸå­—ç¬¦ä¸²
                if stripped.endswith('",') or stripped.endswith('"'):
                    in_string = False
                    fixed_lines.append(current_line)
                    current_line = ""
            else:
                # æ£€æŸ¥æ˜¯å¦å¼€å§‹ä¸€ä¸ªå¯èƒ½è·¨è¡Œçš„å­—ç¬¦ä¸²
                if (': "' in stripped and 
                    not (stripped.endswith('",') or stripped.endswith('"')) and
                    not stripped.endswith('": "') and
                    len(stripped) > 50):  # é•¿å­—ç¬¦ä¸²æ›´å¯èƒ½è·¨è¡Œ
                    in_string = True
                    current_line = line
                else:
                    fixed_lines.append(line)
        
        return '\n'.join(fixed_lines)

    def fix_json_newlines(self, json_str: str) -> str:
        """
        ä¿®å¤JSONå­—ç¬¦ä¸²ä¸­çš„æ¢è¡Œç¬¦é—®é¢˜
        
        Args:
            json_str: å¯èƒ½åŒ…å«æœªè½¬ä¹‰æ¢è¡Œç¬¦çš„JSONå­—ç¬¦ä¸²
            
        Returns:
            ä¿®å¤åçš„JSONå­—ç¬¦ä¸²
        """
        import re
        
        # åœ¨å­—ç¬¦ä¸²å€¼ä¸­æŸ¥æ‰¾æœªè½¬ä¹‰çš„æ¢è¡Œç¬¦å¹¶æ›¿æ¢ä¸ºç©ºæ ¼
        # è¿™ä¸ªæ­£åˆ™è¡¨è¾¾å¼åŒ¹é…åœ¨åŒå¼•å·å†…çš„æ¢è¡Œç¬¦
        def replace_newlines_in_strings(match):
            content = match.group(0)
            # å°†å­—ç¬¦ä¸²å†…çš„æ¢è¡Œç¬¦æ›¿æ¢ä¸ºç©ºæ ¼
            content = content.replace('\n', ' ')
            # å°†å¤šä¸ªç©ºæ ¼åˆå¹¶ä¸ºä¸€ä¸ª
            content = re.sub(r'\s+', ' ', content)
            return content
        
        # åŒ¹é…åŒå¼•å·å†…çš„å†…å®¹ï¼ˆåŒ…æ‹¬è½¬ä¹‰çš„å¼•å·ï¼‰
        pattern = r'"[^"\\]*(?:\\.[^"\\]*)*"'
        fixed_json = re.sub(pattern, replace_newlines_in_strings, json_str)
        
        return fixed_json

    def extract_json_from_output(self, output: str) -> Optional[str]:
        """
        ä»è¾“å‡ºä¸­æå–JSONå†…å®¹ï¼Œæ”¯æŒå¤šç§æ ¼å¼
        
        Args:
            output: Q Chatçš„åŸå§‹è¾“å‡º
            
        Returns:
            æå–çš„JSONå­—ç¬¦ä¸²ï¼Œå¦‚æœæ²¡æ‰¾åˆ°åˆ™è¿”å›None
        """
        import re
        
        # æ›´å¼ºçš„ANSIæ¸…ç†ï¼šå¤„ç†æ‰€æœ‰å¯èƒ½çš„ANSIè½¬ä¹‰åºåˆ—
        # 1. æ¸…ç†Unicodeè½¬ä¹‰çš„ANSIåºåˆ— (\u001b[...)
        output = re.sub(r'\\u001b\[[0-9;]*[a-zA-Z]?', '', output)
        
        # 2. æ¸…ç†è¿ç»­çš„ANSIé‡ç½®åºåˆ—
        output = re.sub(r'(\\u001b\[0m)+', '', output)
        output = re.sub(r'(\\u001b\[m)+', '', output)
        
        # 3. æ¸…ç†å®é™…çš„ANSIè½¬ä¹‰åºåˆ— (\x1B[...)
        ansi_escape = re.compile(r'\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])')
        output = ansi_escape.sub('', output)
        
        # 4. æ¸…ç†å…¶ä»–æ§åˆ¶å­—ç¬¦å’ŒUnicodeæ§åˆ¶å­—ç¬¦
        output = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', output)
        
        # 5. æ¸…ç†å¯èƒ½æ®‹ç•™çš„ANSIç›¸å…³å­—ç¬¦å’Œæç¤ºç¬¦ï¼ˆä½†ä¿ç•™JSONå¼€å¤´çš„>ï¼‰
        output = re.sub(r'\[0m|\[m', '', output)
        
        # 6. æ¸…ç†å¼€å¤´çš„ç©ºç™½ï¼Œä½†ä¿ç•™å¯èƒ½çš„JSONæ ‡è®°
        output = output.strip()
        
        # 7. å¦‚æœä»¥ "> {" å¼€å¤´ï¼Œç§»é™¤å¼€å¤´çš„ "> "
        if output.startswith('> {'):
            output = output[2:].strip()
        
        # 8. é¢å¤–æ¸…ç†ï¼šå¤„ç†ç‰¹æ®Šçš„ANSIæ¨¡å¼
        output = re.sub(r'\\u001b\[[0-9;]*m', '', output)  # å¤„ç†æ‰€æœ‰mç»“å°¾çš„åºåˆ—
        output = re.sub(r'\\u001b\[', '', output)  # æ¸…ç†æ®‹ç•™çš„å¼€å§‹æ ‡è®°
        
        print(f"ğŸ§¹ ANSIæ¸…ç†åçš„è¾“å‡ºé•¿åº¦: {len(output)}")
        
        # æ–¹æ³•0: ç›´æ¥æ£€æµ‹ä»¥ "> {" å¼€å¤´çš„JSON
        if output.startswith('> {'):
            potential_json = output[2:].strip()
            # ä¿®å¤JSONä¸­çš„æ¢è¡Œç¬¦é—®é¢˜
            potential_json = self.fix_json_newlines(potential_json)
            try:
                json.loads(potential_json)
                print("âœ… ç›´æ¥JSONæ£€æµ‹æˆåŠŸï¼ˆ> å‰ç¼€ï¼‰")
                return potential_json
            except json.JSONDecodeError:
                print("âŒ ç›´æ¥JSONæ£€æµ‹å¤±è´¥ï¼ˆ> å‰ç¼€ï¼‰")
        
        # æ–¹æ³•1: å¯»æ‰¾markdownä»£ç å—ä¸­çš„JSON (æ”¹è¿›çš„æ­£åˆ™è¡¨è¾¾å¼)
        json_block_pattern = r'```(?:json)?\s*(\{.*?\})\s*```'
        json_blocks = re.findall(json_block_pattern, output, re.DOTALL | re.IGNORECASE)
        
        if json_blocks:
            print(f"ğŸ“¦ æ‰¾åˆ° {len(json_blocks)} ä¸ªJSONä»£ç å—")
            # å°è¯•è§£ææ¯ä¸ªæ‰¾åˆ°çš„JSONå—ï¼Œè¿”å›ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„
            for i, block in enumerate(reversed(json_blocks)):  # ä»æœ€åä¸€ä¸ªå¼€å§‹å°è¯•
                try:
                    json.loads(block)  # éªŒè¯JSONæœ‰æ•ˆæ€§
                    print(f"âœ… JSONä»£ç å— {len(json_blocks)-i} éªŒè¯æˆåŠŸ")
                    return block
                except json.JSONDecodeError as e:
                    print(f"âŒ JSONä»£ç å— {len(json_blocks)-i} éªŒè¯å¤±è´¥: {e}")
                    continue
        
        # æ–¹æ³•2: å¯»æ‰¾æœ€å¤§çš„å®Œæ•´JSONå¯¹è±¡ (æ”¹è¿›çš„æ‹¬å·åŒ¹é…)
        json_start = output.find('{')
        if json_start == -1:
            print("âŒ æœªæ‰¾åˆ°JSONèµ·å§‹æ ‡è®°")
            return None
            
        print(f"ğŸ” æ‰¾åˆ°JSONèµ·å§‹ä½ç½®: {json_start}")
        
        # æ‰¾åˆ°åŒ¹é…çš„ç»“æŸæ‹¬å·ï¼Œå¤„ç†å­—ç¬¦ä¸²ä¸­çš„æ‹¬å·
        brace_count = 0
        in_string = False
        escape_next = False
        json_end = json_start
        
        for i in range(json_start, len(output)):
            char = output[i]
            
            if escape_next:
                escape_next = False
                continue
                
            if char == '\\':
                escape_next = True
                continue
                
            if char == '"' and not escape_next:
                in_string = not in_string
                continue
                
            if not in_string:
                if char == '{':
                    brace_count += 1
                elif char == '}':
                    brace_count -= 1
                    if brace_count == 0:
                        json_end = i + 1
                        break
        
        if brace_count == 0:
            candidate = output[json_start:json_end]
            try:
                json.loads(candidate)  # éªŒè¯JSONæœ‰æ•ˆæ€§
                return candidate
            except json.JSONDecodeError:
                pass
        
        # æ–¹æ³•3: å¯»æ‰¾å¤šè¡ŒJSONç»“æ„ (å¤„ç†æ ¼å¼åŒ–çš„JSON)
        lines = output.split('\n')
        json_lines = []
        in_json = False
        brace_count = 0
        
        for line in lines:
            stripped = line.strip()
            if not in_json and stripped.startswith('{'):
                in_json = True
                json_lines = [line]
                brace_count = stripped.count('{') - stripped.count('}')
            elif in_json:
                json_lines.append(line)
                brace_count += stripped.count('{') - stripped.count('}')
                if brace_count <= 0:
                    break
        
        if json_lines and brace_count <= 0:
            candidate = '\n'.join(json_lines)
            try:
                json.loads(candidate)  # éªŒè¯JSONæœ‰æ•ˆæ€§
                return candidate
            except json.JSONDecodeError:
                pass
        
        return None

    def extract_clean_result(self, result: Dict[str, Any]) -> Any:
        """
        ä»Q Chatç»“æœä¸­æå–å¹²å‡€çš„åˆ†ææ•°æ®
        
        Args:
            result: Q Chatè¿”å›çš„ç»“æœå¯¹è±¡
            
        Returns:
            æå–çš„å¹²å‡€æ•°æ®ï¼Œå¦‚æœæå–å¤±è´¥åˆ™è¿”å›None
        """
        if not isinstance(result, dict):
            return result
        
        # å¦‚æœæœ‰é”™è¯¯ä½†ä¹Ÿæœ‰raw_outputï¼Œå°è¯•ä»raw_outputä¸­æå–JSON
        if 'error' in result:
            logger.warning(f"ç»“æœåŒ…å«é”™è¯¯: {result.get('error')}")
            if 'raw_output' in result:
                logger.info("å°è¯•ä»raw_outputä¸­æå–JSON...")
                raw_output = result['raw_output']
                
                # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœraw_outputä»¥ "> {" å¼€å¤´ï¼Œç›´æ¥æå–JSONéƒ¨åˆ†
                if raw_output.startswith('> {'):
                    json_content = raw_output[2:].strip()
                    
                    # å¤„ç†è½¬ä¹‰çš„JSONï¼šå°† \\\" æ›¿æ¢ä¸º \"ï¼Œå°† \\n æ›¿æ¢ä¸ºå®é™…æ¢è¡Œ
                    json_content = json_content.replace('\\"', '"').replace('\\n', '\n')
                    
                    # ä¿®å¤å¤šè¡Œå­—ç¬¦ä¸²é—®é¢˜ï¼šå°†å­—ç¬¦ä¸²ä¸­çš„æ¢è¡Œç¬¦æ›¿æ¢ä¸ºç©ºæ ¼
                    json_content = self.fix_multiline_json_strings(json_content)
                    
                    try:
                        extracted_data = json.loads(json_content)
                        logger.info("âœ… æˆåŠŸä»raw_outputä¸­æå–å¹¶ä¿®å¤JSONæ•°æ®")
                        return extracted_data
                    except json.JSONDecodeError as e:
                        logger.warning(f"âŒ ç›´æ¥è§£æå¤±è´¥: {e}")
                        # ä¿å­˜è°ƒè¯•ä¿¡æ¯
                        with open('debug_json_content.txt', 'w', encoding='utf-8') as f:
                            f.write(json_content)
                        logger.info("è°ƒè¯•ä¿¡æ¯å·²ä¿å­˜åˆ° debug_json_content.txt")
                
                # å¦‚æœç›´æ¥è§£æå¤±è´¥ï¼Œä½¿ç”¨åŸæœ‰çš„æå–æ–¹æ³•
                json_str = self.extract_json_from_output(raw_output)
                if json_str:
                    try:
                        extracted_data = json.loads(json_str)
                        logger.info("âœ… æˆåŠŸä»raw_outputä¸­æå–JSONæ•°æ®")
                        return extracted_data
                    except json.JSONDecodeError:
                        logger.warning("âŒ æ— æ³•ä»raw_outputä¸­è§£æJSON")
            return None
        
        # å¦‚æœæœ‰raw_outputä½†æ²¡æœ‰å…¶ä»–ç»“æ„åŒ–æ•°æ®ï¼Œå°è¯•ä»raw_outputä¸­æå–JSON
        if 'raw_output' in result and len(result) == 1:
            raw_output = result['raw_output']
            json_str = self.extract_json_from_output(raw_output)
            if json_str:
                try:
                    return json.loads(json_str)
                except json.JSONDecodeError:
                    logger.warning("æ— æ³•ä»raw_outputä¸­è§£æJSON")
                    return None
            return None
        
        # å¦‚æœç»“æœä¸­æœ‰raw_outputä½†è¿˜æœ‰å…¶ä»–å­—æ®µï¼Œç§»é™¤raw_outputè¿”å›å…¶ä»–å­—æ®µ
        if 'raw_output' in result:
            clean_result = {k: v for k, v in result.items() if k != 'raw_output'}
            if clean_result:
                return clean_result
        
        # ç›´æ¥è¿”å›ç»“æœ
        return result

    def prepare_context_data(self, context_data: Dict) -> Dict:
        """
        å‡†å¤‡ä¸Šä¸‹æ–‡æ•°æ®ï¼Œä¼˜åŒ–JSONåºåˆ—åŒ–
        
        Args:
            context_data: åŸå§‹ä¸Šä¸‹æ–‡æ•°æ®
            
        Returns:
            ä¼˜åŒ–åçš„ä¸Šä¸‹æ–‡æ•°æ®
        """
        optimized_data = {}
        
        for key, value in context_data.items():
            if value is None:
                # å¯¹äºNoneå€¼ï¼Œæä¾›ä¸€ä¸ªè¯´æ˜æ€§çš„å ä½ç¬¦
                optimized_data[key] = f"[{key}åˆ†æç»“æœæœªèƒ½æˆåŠŸæå–ï¼Œè¯·æ£€æŸ¥å‰åºæ­¥éª¤]"
                logger.warning(f"å‚æ•° {key} ä¸ºNoneï¼Œä½¿ç”¨å ä½ç¬¦")
            elif isinstance(value, pd.DataFrame):
                # DataFrameè½¬æ¢ä¸ºç´§å‡‘çš„å­—å…¸åˆ—è¡¨
                optimized_data[key] = value.to_dict('records')
            elif isinstance(value, list) and len(value) > 0:
                # ä¿æŒåˆ—è¡¨æ•°æ®å®Œæ•´
                optimized_data[key] = value
            elif isinstance(value, dict):
                # é€’å½’å¤„ç†åµŒå¥—å­—å…¸ï¼Œä½†è¦æ£€æŸ¥æ˜¯å¦åŒ…å«é”™è¯¯ä¿¡æ¯
                if 'error' in value:
                    optimized_data[key] = f"[{key}åˆ†æå¤±è´¥: {value.get('error')}]"
                    logger.warning(f"å‚æ•° {key} åŒ…å«é”™è¯¯ä¿¡æ¯")
                else:
                    optimized_data[key] = self.prepare_context_data(value) if any(isinstance(v, (pd.DataFrame, dict)) for v in value.values()) else value
            else:
                optimized_data[key] = value
        
        return optimized_data
    
    def run_analysis_pipeline(self, customer_review_path: str, competitor_review_path: str, product_type: str) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´çš„åˆ†æç®¡é“
        
        Args:
            customer_review_path: å®¢æˆ·è¯„è®ºCSVæ–‡ä»¶è·¯å¾„
            competitor_review_path: ç«äº‰å¯¹æ‰‹è¯„è®ºCSVæ–‡ä»¶è·¯å¾„
            product_type: äº§å“ç±»å‹ä¿¡æ¯
            
        Returns:
            æ‰€æœ‰åˆ†æç»“æœçš„å­—å…¸
        """
        logger.info("å¼€å§‹è¿è¡Œåˆ†æç®¡é“...")
        
        # 1. æ•°æ®æ¸…ç†
        self.load_and_clean_data(customer_review_path, competitor_review_path)
        
        # 2. äº§å“ç±»å‹åˆ†æ
        logger.info("æ­¥éª¤1: äº§å“ç±»å‹åˆ†æ")
        product_type_prompt = self.load_prompt('product_type.md')
        self.results['product_type'] = self.call_q_chat(
            product_type_prompt, 
            {'product_type': product_type}
        )
        # ä¿å­˜ç¬¬ä¸€æ­¥ç»“æœ
        step_file = self.output_dir / "product_type.json"
        with open(step_file, 'w', encoding='utf-8') as f:
            json.dump(self.results['product_type'], f, indent=2, ensure_ascii=False)
        logger.info(f"æ­¥éª¤1ç»“æœå·²ä¿å­˜: {step_file}")
        
        # æå–ç¬¬ä¸€æ­¥çš„å¹²å‡€ç»“æœç”¨äºåç»­æ­¥éª¤
        clean_product_type = self.extract_clean_result(self.results['product_type'])
        
        # 3. æ¶ˆè´¹è€…åˆ†æ (5ä¸ªå¹¶è¡Œæ­¥éª¤)
        consumer_prompts = [
            'consumer_profile.md',
            'consumer_scenario.md', 
            'consumer_motivation.md',
            'consumer_love.md',
            'unmet_needs.md'
        ]
        
        logger.info("æ­¥éª¤2: æ¶ˆè´¹è€…åˆ†æ")
        for prompt_file in consumer_prompts:
            prompt_name = prompt_file.replace('.md', '')
            logger.info(f"  æ‰§è¡Œ: {prompt_name}")
            
            prompt = self.load_prompt(prompt_file)
            
            # ä¸ºæ¯ä¸ªæ¶ˆè´¹è€…åˆ†ææä¾›ç›¸åº”çš„ä¸Šä¸‹æ–‡ - ä½¿ç”¨ç¬¬ä¸€æ­¥çš„JSONç»“æœ
            context = {
                'product_type': clean_product_type if clean_product_type else product_type,  # ä¼˜å…ˆä½¿ç”¨JSONç»“æœï¼Œfallbackåˆ°å­—ç¬¦ä¸²
                'customer_review_data': self.cleaned_data['customer_review']
            }
            
            # ä¼˜åŒ–ä¸Šä¸‹æ–‡æ•°æ®
            optimized_context = self.prepare_context_data(context)
            self.results[prompt_name] = self.call_q_chat(prompt, optimized_context)
            
            # ä¿å­˜æ¯ä¸ªæ¶ˆè´¹è€…åˆ†ææ­¥éª¤çš„ç»“æœ
            step_file = self.output_dir / f"{prompt_name}.json"
            with open(step_file, 'w', encoding='utf-8') as f:
                json.dump(self.results[prompt_name], f, indent=2, ensure_ascii=False)
            logger.info(f"æ­¥éª¤2.{prompt_name}ç»“æœå·²ä¿å­˜: {step_file}")
        
        # 4. æœºä¼šåˆ†æ (ä¿®å¤ä¾èµ–é—®é¢˜)
        logger.info("æ­¥éª¤3: æœºä¼šåˆ†æ")
        opportunity_prompt = self.load_prompt('opportunity.md')
        
        # æå–å¹²å‡€çš„åˆ†æç»“æœç”¨äºå‚æ•°ä¼ é€’
        clean_consumer_love = self.extract_clean_result(self.results['consumer_love'])
        clean_unmet_needs = self.extract_clean_result(self.results['unmet_needs'])
        clean_consumer_scenario = self.extract_clean_result(self.results['consumer_scenario'])
        
        opportunity_context = {
            'product_type': clean_product_type if clean_product_type else product_type,  # ä½¿ç”¨ç¬¬ä¸€æ­¥JSONç»“æœ
            'consumer_love': clean_consumer_love if clean_consumer_love else "[æ¶ˆè´¹è€…å–œçˆ±ç‚¹åˆ†æä¸å¯ç”¨]",
            'unmet_needs': clean_unmet_needs if clean_unmet_needs else "[æœªæ»¡è¶³éœ€æ±‚åˆ†æä¸å¯ç”¨]",
            'consumer_scenario': clean_consumer_scenario if clean_consumer_scenario else "[ä½¿ç”¨åœºæ™¯åˆ†æä¸å¯ç”¨]",
            'customer_review_data': self.cleaned_data['customer_review']
        }
        self.results['opportunity'] = self.call_q_chat(opportunity_prompt, self.prepare_context_data(opportunity_context))
        # ä¿å­˜æœºä¼šåˆ†æç»“æœ
        step_file = self.output_dir / "opportunity.json"
        with open(step_file, 'w', encoding='utf-8') as f:
            json.dump(self.results['opportunity'], f, indent=2, ensure_ascii=False)
        logger.info(f"æ­¥éª¤3ç»“æœå·²ä¿å­˜: {step_file}")
        
        # 5. æ˜Ÿçº§è¯„åˆ†æ ¹å› åˆ†æ (ä¿®å¤ä¾èµ–é—®é¢˜)
        logger.info("æ­¥éª¤4: æ˜Ÿçº§è¯„åˆ†æ ¹å› åˆ†æ")
        star_rating_prompt = self.load_prompt('star_rating_root_cause.md')
        star_rating_context = {
            'product_type': clean_product_type if clean_product_type else product_type,  # ä½¿ç”¨ç¬¬ä¸€æ­¥JSONç»“æœ
            'consumer_love': clean_consumer_love if clean_consumer_love else "[æ¶ˆè´¹è€…å–œçˆ±ç‚¹åˆ†æä¸å¯ç”¨]",
            'unmet_needs': clean_unmet_needs if clean_unmet_needs else "[æœªæ»¡è¶³éœ€æ±‚åˆ†æä¸å¯ç”¨]",
            'customer_review_data': self.cleaned_data['customer_review']
        }
        self.results['star_rating_root_cause'] = self.call_q_chat(star_rating_prompt, self.prepare_context_data(star_rating_context))
        # ä¿å­˜æ˜Ÿçº§è¯„åˆ†åˆ†æç»“æœ
        step_file = self.output_dir / "star_rating_root_cause.json"
        with open(step_file, 'w', encoding='utf-8') as f:
            json.dump(self.results['star_rating_root_cause'], f, indent=2, ensure_ascii=False)
        logger.info(f"æ­¥éª¤4ç»“æœå·²ä¿å­˜: {step_file}")
        
        # 6. ç«äº‰å¯¹æ‰‹åˆ†æ (æ–°çš„ä¸‰é˜¶æ®µæµç¨‹)
        logger.info("æ­¥éª¤5: ç«äº‰å¯¹æ‰‹åˆ†æ")
        
        # æå–æˆ‘æ–¹ç»´åº¦æ¸…å•
        clean_consumer_love = self.extract_clean_result(self.results['consumer_love'])
        clean_unmet_needs = self.extract_clean_result(self.results['unmet_needs'])
        clean_consumer_motivation = self.extract_clean_result(self.results['consumer_motivation'])
        
        if clean_consumer_love or clean_unmet_needs or clean_consumer_motivation:
            # æå–ç»´åº¦åˆ—è¡¨ï¼ˆåªä»æˆåŠŸçš„æ¨¡å—ä¸­æå–ï¼‰
            our_love_dimensions = []
            our_unmet_dimensions = []
            our_motivation_dimensions = []
            
            if clean_consumer_love:
                our_love_dimensions = [item["èµç¾ç‚¹"] for item in clean_consumer_love.get("æ ¸å¿ƒèµç¾ç‚¹åˆ†æ", [])]
            if clean_unmet_needs:
                our_unmet_dimensions = [item["éœ€æ±‚ç±»å‹"] for item in clean_unmet_needs.get("æœªæ»¡è¶³éœ€æ±‚åˆ†æ", [])]
            if clean_consumer_motivation:
                our_motivation_dimensions = [item["åŠ¨æœº"] for item in clean_consumer_motivation.get("å…·ä½“è´­ä¹°åŠ¨æœº", [])]
            
            logger.info(f"  æå–ç»´åº¦: å–œçˆ±ç‚¹{len(our_love_dimensions)}ä¸ª, æœªæ»¡è¶³éœ€æ±‚{len(our_unmet_dimensions)}ä¸ª, è´­ä¹°åŠ¨æœº{len(our_motivation_dimensions)}ä¸ª")
            
            # é˜¶æ®µ1: ç«å“åŸºç¡€åˆ†æ
            logger.info("  é˜¶æ®µ1: ç«å“åŸºç¡€åˆ†æ")
            competitor_base_prompt = self.load_prompt('competitor_analysis_base.md')
            competitor_base_context = {
                'our_love_dimensions': our_love_dimensions,
                'our_unmet_dimensions': our_unmet_dimensions,
                'our_motivation_dimensions': our_motivation_dimensions,
                'competitor_review_data': self.cleaned_data['competitor_review']
            }
            self.results['competitor_base'] = self.call_q_chat(competitor_base_prompt, self.prepare_context_data(competitor_base_context))
            
            # ä¿å­˜ç«å“åŸºç¡€åˆ†æç»“æœ
            step_file = self.output_dir / "competitor_base.json"
            with open(step_file, 'w', encoding='utf-8') as f:
                json.dump(self.results['competitor_base'], f, indent=2, ensure_ascii=False)
            logger.info(f"  é˜¶æ®µ1ç»“æœå·²ä¿å­˜: {step_file}")
            
            # é˜¶æ®µ2: ç«å“å¯¹æ¯”åˆ†æ
            logger.info("  é˜¶æ®µ2: ç«å“å¯¹æ¯”åˆ†æ")
            clean_competitor_base = self.extract_clean_result(self.results['competitor_base'])
            
            if clean_competitor_base:
                competitor_comparison_prompt = self.load_prompt('competitor_comparison.md')
                competitor_comparison_context = {
                    'our_consumer_love': clean_consumer_love or {"æ ¸å¿ƒèµç¾ç‚¹åˆ†æ": []},
                    'our_unmet_needs': clean_unmet_needs or {"æœªæ»¡è¶³éœ€æ±‚åˆ†æ": []},
                    'our_consumer_motivation': clean_consumer_motivation or {"å…·ä½“è´­ä¹°åŠ¨æœº": []},
                    'competitor_consumer_love': clean_competitor_base.get('ç«å“æ¶ˆè´¹è€…å–œçˆ±ç‚¹', []),
                    'competitor_unmet_needs': clean_competitor_base.get('ç«å“æœªæ»¡è¶³éœ€æ±‚', []),
                    'competitor_consumer_motivation': clean_competitor_base.get('ç«å“è´­ä¹°åŠ¨æœº', [])
                }
                self.results['competitor_comparison'] = self.call_q_chat(competitor_comparison_prompt, self.prepare_context_data(competitor_comparison_context))
                
                # ä¿å­˜ç«å“å¯¹æ¯”åˆ†æç»“æœ
                step_file = self.output_dir / "competitor_comparison.json"
                with open(step_file, 'w', encoding='utf-8') as f:
                    json.dump(self.results['competitor_comparison'], f, indent=2, ensure_ascii=False)
                logger.info(f"  é˜¶æ®µ2ç»“æœå·²ä¿å­˜: {step_file}")
            else:
                logger.warning("  ç«å“åŸºç¡€åˆ†æå¤±è´¥ï¼Œè·³è¿‡å¯¹æ¯”åˆ†æ")
                self.results['competitor_comparison'] = {"error": "ç«å“åŸºç¡€åˆ†æå¤±è´¥"}
            
            # é˜¶æ®µ3: ç«å“ç‹¬æœ‰æ´å¯Ÿ
            logger.info("  é˜¶æ®µ3: ç«å“ç‹¬æœ‰æ´å¯Ÿ")
            competitor_unique_prompt = self.load_prompt('competitor_unique_insights.md')
            all_our_dimensions = our_love_dimensions + our_unmet_dimensions + our_motivation_dimensions
            competitor_unique_context = {
                'competitor_review_data': self.cleaned_data['competitor_review'],
                'our_analyzed_dimensions': all_our_dimensions
            }
            self.results['competitor_unique'] = self.call_q_chat(competitor_unique_prompt, self.prepare_context_data(competitor_unique_context))
            
            # ä¿å­˜ç«å“ç‹¬æœ‰æ´å¯Ÿç»“æœ
            step_file = self.output_dir / "competitor_unique.json"
            with open(step_file, 'w', encoding='utf-8') as f:
                json.dump(self.results['competitor_unique'], f, indent=2, ensure_ascii=False)
            logger.info(f"  é˜¶æ®µ3ç»“æœå·²ä¿å­˜: {step_file}")
            
            # åˆå¹¶æœ€ç»ˆç«å“åˆ†æç»“æœ
            final_competitor_result = {
                "ç«å“åŸºç¡€åˆ†æ": clean_competitor_base if clean_competitor_base else {"error": "åˆ†æå¤±è´¥"},
                "ç«å“å¯¹æ¯”åˆ†æ": self.extract_clean_result(self.results['competitor_comparison']) or {"error": "åˆ†æå¤±è´¥"},
                "ç«å“ç‹¬æœ‰æ´å¯Ÿ": self.extract_clean_result(self.results['competitor_unique']) or {"error": "åˆ†æå¤±è´¥"}
            }
            self.results['competitor'] = final_competitor_result
            
        else:
            logger.warning("æˆ‘æ–¹åŸºç¡€åˆ†æå…¨éƒ¨å¤±è´¥ï¼Œè·³è¿‡ç«å“åˆ†æ")
            self.results['competitor'] = {"error": "æˆ‘æ–¹åŸºç¡€åˆ†æå…¨éƒ¨å¤±è´¥ï¼Œæ— æ³•è¿›è¡Œç«å“å¯¹æ¯”"}
        
        # ä¿å­˜æœ€ç»ˆç«å“åˆ†æç»“æœ
        step_file = self.output_dir / "competitor.json"
        with open(step_file, 'w', encoding='utf-8') as f:
            json.dump(self.results['competitor'], f, indent=2, ensure_ascii=False)
        logger.info(f"æ­¥éª¤5ç»“æœå·²ä¿å­˜: {step_file}")
        
        logger.info("åˆ†æç®¡é“å®Œæˆ!")
        return self.results
    
    def save_results(self) -> str:
        """
        ä¿å­˜æ‰€æœ‰åˆ†æç»“æœåˆ°æ—¶é—´æˆ³è¾“å‡ºç›®å½•
        
        Returns:
            è¾“å‡ºç›®å½•è·¯å¾„
        """
        # ä¿å­˜å®Œæ•´ç»“æœ
        results_file = self.output_dir / "analysis_results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜æ¯ä¸ªæ­¥éª¤çš„å•ç‹¬æ–‡ä»¶
        for step_name, result in self.results.items():
            step_file = self.output_dir / f"{step_name}.json"
            with open(step_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, indent=2, ensure_ascii=False)
        
        logger.info(f"æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {self.output_dir}")
        return str(self.output_dir)

def main():
    """ä¸»å‡½æ•° - å‘½ä»¤è¡Œæ¥å£"""
    if len(sys.argv) != 4:
        print("ä½¿ç”¨æ–¹æ³•: python review_analyzer.py <customer_review.csv> <competitor_review.csv> <product_type>")
        sys.exit(1)
    
    customer_review_path = sys.argv[1]
    competitor_review_path = sys.argv[2]
    product_type = sys.argv[3]
    
    try:
        analyzer = ReviewAnalyzer()
        results = analyzer.run_analysis_pipeline(customer_review_path, competitor_review_path, product_type)
        output_dir = analyzer.save_results()
        
        print(f"\nâœ… åˆ†æå®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
        print(f"ğŸ“Š å…±å®Œæˆ {len(results)} ä¸ªåˆ†ææ­¥éª¤")
        
        # æ˜¾ç¤ºæ‰§è¡Œæ‘˜è¦
        success_count = sum(1 for r in results.values() if not (isinstance(r, dict) and 'error' in r))
        print(f"âœ… æˆåŠŸ: {success_count}/{len(results)} ä¸ªæ­¥éª¤")
        
        print("\nğŸ“‹ åˆ†ææ­¥éª¤:")
        for step in results.keys():
            result = results[step]
            if isinstance(result, dict) and 'error' in result:
                print(f"  âŒ {step}")
            else:
                print(f"  âœ… {step}")
            
    except Exception as e:
        logger.error(f"åˆ†æå¤±è´¥: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    main()
